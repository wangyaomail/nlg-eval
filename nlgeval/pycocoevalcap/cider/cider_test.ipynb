{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange as range\n",
    "import six\n",
    "\n",
    "def precook(s, n=4, out=False):\n",
    "    words = s.split()\n",
    "    counts = defaultdict(int)\n",
    "    for k in range(1, n + 1):\n",
    "        for i in range(len(words) - k + 1):\n",
    "            ngram = tuple(words[i:i + k])\n",
    "            counts[ngram] += 1\n",
    "    return counts\n",
    "\n",
    "def cook_refs(refs, n=4):  ## lhuang: oracle will call with \"average\"\n",
    "    return [precook(ref, n) for ref in refs]\n",
    "\n",
    "def cook_test(test, n=4):\n",
    "    return precook(test, n, True)\n",
    "\n",
    "class CiderScorer(object):\n",
    "    def copy(self):\n",
    "        new = CiderScorer(n=self.n)\n",
    "        new.ctest = copy.copy(self.ctest)\n",
    "        new.crefs = copy.copy(self.crefs)\n",
    "        return new\n",
    "    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n",
    "        ''' singular instance '''\n",
    "        self.n = n\n",
    "        self.sigma = sigma\n",
    "        self.crefs = []\n",
    "        self.ctest = []\n",
    "        self.document_frequency = defaultdict(float)\n",
    "        self.cook_append(test, refs)\n",
    "        self.ref_len = None\n",
    "    def cook_append(self, test, refs):\n",
    "        '''called by constructor and __iadd__ to avoid creating new instances.'''\n",
    "\n",
    "        if refs is not None:\n",
    "            self.crefs.append(cook_refs(refs))\n",
    "            if test is not None:\n",
    "                self.ctest.append(cook_test(test))  ## N.B.: -1\n",
    "            else:\n",
    "                self.ctest.append(None)  # lens of crefs and ctest have to match\n",
    "    def size(self):\n",
    "        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n",
    "        return len(self.crefs)\n",
    "    def __iadd__(self, other):\n",
    "        '''add an instance (e.g., from another sentence).'''\n",
    "\n",
    "        if type(other) is tuple:\n",
    "            ## avoid creating new CiderScorer instances\n",
    "            self.cook_append(other[0], other[1])\n",
    "        else:\n",
    "            self.ctest.extend(other.ctest)\n",
    "            self.crefs.extend(other.crefs)\n",
    "\n",
    "        return self\n",
    "    def compute_doc_freq(self):\n",
    "        for refs in self.crefs:\n",
    "            # refs, k ref captions of one image\n",
    "            for ngram in set([ngram for ref in refs for (ngram, count) in six.iteritems(ref)]):\n",
    "                self.document_frequency[ngram] += 1\n",
    "            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n",
    "    def compute_cider(self):\n",
    "        def counts2vec(cnts):\n",
    "            vec = [defaultdict(float) for _ in range(self.n)]\n",
    "            length = 0\n",
    "            norm = [0.0 for _ in range(self.n)]\n",
    "            for (ngram, term_freq) in six.iteritems(cnts):\n",
    "                # give word count 1 if it doesn't appear in reference corpus\n",
    "                df = np.log(max(1.0, self.document_frequency[ngram]))\n",
    "                # ngram index\n",
    "                n = len(ngram) - 1\n",
    "                # tf (term_freq) * idf (precomputed idf) for n-grams\n",
    "                vec[n][ngram] = float(term_freq) * (self.ref_len - df)\n",
    "                # compute norm for the vector.  the norm will be used for computing similarity\n",
    "                norm[n] += pow(vec[n][ngram], 2)\n",
    "\n",
    "                if n == 1:\n",
    "                    length += term_freq\n",
    "            norm = [np.sqrt(n) for n in norm]\n",
    "            return vec, norm, length\n",
    "        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n",
    "            delta = float(length_hyp - length_ref)\n",
    "            val = np.array([0.0 for _ in range(self.n)])\n",
    "            for n in range(self.n):\n",
    "                # ngram\n",
    "                for (ngram, count) in six.iteritems(vec_hyp[n]):\n",
    "                    # vrama91 : added clipping\n",
    "                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n",
    "\n",
    "                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n",
    "                    val[n] /= (norm_hyp[n] * norm_ref[n])\n",
    "\n",
    "                assert (not math.isnan(val[n]))\n",
    "                # vrama91: added a length based gaussian penalty\n",
    "                val[n] *= np.e ** (-(delta ** 2) / (2 * self.sigma ** 2))\n",
    "            return val\n",
    "        # compute log reference length\n",
    "        self.ref_len = np.log(float(len(self.crefs)))\n",
    "\n",
    "        scores = []\n",
    "        for test, refs in zip(self.ctest, self.crefs):\n",
    "            # compute vector for test captions\n",
    "            vec, norm, length = counts2vec(test)\n",
    "            # compute vector for ref captions\n",
    "            score = np.array([0.0 for _ in range(self.n)])\n",
    "            for ref in refs:\n",
    "                vec_ref, norm_ref, length_ref = counts2vec(ref)\n",
    "                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n",
    "            # change by vrama91 - mean of ngram scores, instead of sum\n",
    "            score_avg = np.mean(score)\n",
    "            # divide by number of references\n",
    "            score_avg /= len(refs)\n",
    "            # multiply score by 10\n",
    "            score_avg *= 10.0\n",
    "            # append score of an image to the score list\n",
    "            scores.append(score_avg)\n",
    "        return scores\n",
    "    def compute_score(self, option=None, verbose=0):\n",
    "        # compute idf\n",
    "        self.compute_doc_freq()\n",
    "        # assert to check document frequency\n",
    "        assert (len(self.ctest) >= max(self.document_frequency.values()))\n",
    "        # compute cider score\n",
    "        score = self.compute_cider()\n",
    "        # debug\n",
    "        # print score\n",
    "        return np.mean(np.array(score)), np.array(score)\n",
    "\n",
    "class Cider:\n",
    "    \"\"\"\n",
    "    Main Class to compute the CIDEr metric\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n",
    "        # set cider to sum over 1 to 4-grams\n",
    "        self._n = n\n",
    "        # set the standard deviation parameter for gaussian penalty\n",
    "        self._sigma = sigma\n",
    "    def compute_score(self, gts, res):\n",
    "        \"\"\"\n",
    "        Main function to compute CIDEr score\n",
    "        :param  hypo_for_image (dict) : dictionary with key <image> and value <tokenized hypothesis / candidate sentence>\n",
    "                ref_for_image (dict)  : dictionary with key <image> and value <tokenized reference sentence>\n",
    "        :return: cider (float) : computed CIDEr score for the corpus\n",
    "        \"\"\"\n",
    "\n",
    "        assert (gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n",
    "\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref = gts[id]\n",
    "\n",
    "            # Sanity check.\n",
    "            assert (type(hypo) is list)\n",
    "            assert (len(hypo) == 1)\n",
    "            assert (type(ref) is list)\n",
    "            assert (len(ref) > 0)\n",
    "\n",
    "            cider_scorer += (hypo[0], ref)\n",
    "\n",
    "        (score, scores) = cider_scorer.compute_score()\n",
    "\n",
    "        return score, scores\n",
    "    def method(self):\n",
    "        return \"CIDEr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "refs = {0: [\"this is a test\", \"this is also a test\"]}\n",
    "hyps = {0: [\"this is a good test\"]}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    cider = Cider(i)\n",
    "    score, scores = cider.compute_score(refs, hyps)\n",
    "    print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
